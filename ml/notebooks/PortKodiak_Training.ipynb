{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# PortKodiak AI Shield - Model Training\n",
                "\n",
                "This notebook trains an **Isolation Forest** model to detect anomalous network traffic based on data collected by the PortKodiak agent.\n",
                "\n",
                "## 1. Setup & Upload\n",
                "First, upload your `traffic_export_TIMESTAMP.csv` file using the file uploader on the left."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.ensemble import IsolationForest\n",
                "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
                "from sklearn.compose import ColumnTransformer\n",
                "from sklearn.pipeline import Pipeline\n",
                "from sklearn.feature_extraction.text import HashingVectorizer\n",
                "import joblib\n",
                "\n",
                "# Check files\n",
                "!ls -lh *.csv"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load Data\n",
                "Update the filename below to match your uploaded CSV."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "csv_file = \"traffic_export.csv\" # CHANGE THIS to your actual filename if different\n",
                "\n",
                "try:\n",
                "    df = pd.read_csv(csv_file)\n",
                "    print(f\"Loaded {len(df)} records.\")\n",
                "    display(df.head())\n",
                "except FileNotFoundError:\n",
                "    print(\"File not found! Please upload your CSV.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Preprocessing\n",
                "We need to convert categorical features (like Process Name) into numbers.\n",
                "\n",
                "- **Process Name**: We use `HashingVectorizer` to handle arbitrary process names without exploding dimensionality.\n",
                "- **Ports**: Standard Scaling.\n",
                "- **Direction**: One-Hot Encoding."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Feature Selection\n",
                "# We focus on: remote_port, process_name (hashed), direction\n",
                "# Advanced features (IP geo, time of day) can be added later.\n",
                "\n",
                "class HashingTransformer:\n",
                "    def __init__(self, n_features=32):\n",
                "        self.vec = HashingVectorizer(n_features=n_features, alternate_sign=False, norm=None)\n",
                "    def fit(self, X, y=None):\n",
                "        return self\n",
                "    def transform(self, X):\n",
                "        # X is DataFrame or Series. Flatten to string list.\n",
                "        if isinstance(X, pd.DataFrame):\n",
                "            X = X.iloc[:, 0]\n",
                "        return self.vec.transform(X.astype(str)).toarray()\n",
                "\n",
                "preprocessor = ColumnTransformer(\n",
                "    transformers=[\n",
                "        ('port', StandardScaler(), ['remote_port']),\n",
                "        ('proc', HashingTransformer(n_features=16), ['process_name']),\n",
                "        ('path', HashingTransformer(n_features=32), ['process_path']),\n",
                "    ],\n",
                "    remainder='drop'\n",
                ")\n",
                "\n",
                "# Create Pipeline\n",
                "clf = IsolationForest(n_estimators=100, contamination=0.01, random_state=42, n_jobs=-1)\n",
                "\n",
                "pipeline = Pipeline([\n",
                "    ('preprocessor', preprocessor),\n",
                "    ('classifier', clf)\n",
                "])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Train Model\n",
                "We train on the assumption that most of your data is \"Normal\". Outliers will be flagged."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Training model...\")\n",
                "pipeline.fit(df)\n",
                "print(\"Training Complete!\")\n",
                "\n",
                "# Score the training data to see distribution\n",
                "scores = pipeline.decision_function(df)\n",
                "plt.figure(figsize=(10, 6))\n",
                "sns.histplot(scores, kde=True)\n",
                "plt.title(\"Anomaly Scores Distribution\")\n",
                "plt.xlabel(\"Score (Lower = More Anomalous)\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Evaluate (Anomalies)\n",
                "Let's look at the top anomalies found in your own training set."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df['score'] = scores\n",
                "df['anomaly'] = pipeline.predict(df)\n",
                "\n",
                "# Show top anomalies (score < 0)\n",
                "anomalies = df[df['anomaly'] == -1].sort_values('score')\n",
                "print(f\"Found {len(anomalies)} anomalies in training set.\")\n",
                "display(anomalies[['timestamp', 'process_name', 'remote_ip', 'remote_port', 'score']].head(20))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Export Model\n",
                "Download this `model.pkl` and place it in your `ml/models/` directory."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "joblib.dump(pipeline, \"portkodiak_model.pkl\")\n",
                "print(\"Model saved as portkodiak_model.pkl\")\n",
                "\n",
                "from google.colab import files\n",
                "files.download('portkodiak_model.pkl')"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}